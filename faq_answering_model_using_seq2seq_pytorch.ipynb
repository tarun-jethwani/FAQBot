{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "faq_answering_model_using_seq2seq_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarun-jethwani/FAQBot/blob/master/faq_answering_model_using_seq2seq_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV-aNtGE0ftn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Implementing FAQ bot with a Sequence to Sequence Network using Attention Mechanism\n",
        "************************************************************************************\n",
        "**Source**: `https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#sphx-glr-intermediate-seq2seq-translation-tutorial-py>`_\n",
        "\n",
        "In this project we will be training a neural network to predict Answer of FAQ's asked by users from\n",
        "Amazon AWS EC2 FAQ\n",
        "\n",
        "::\n",
        "\n",
        "    -- > input, = target, < output\n",
        "\n",
        "    > what can developers now do that they could not before\n",
        "    = developers are now free to innovate knowing that no matter how successful their businesses become it will be      inexpensive and simple to ensure they have the compute capacity they need to meet their business requirements .\n",
        "    < developers are now free to innovate knowing that no matter how successful their businesses become it will be inexpensive and simple to ensure they have the compute capacity they need to meet their business requirements . \n",
        "\n",
        "Please visit tutorial page of this Project on my Blog [Leaky ReLU : Practical Guide to Machine Learning, Deep Learning and AI](https://leakyrelu.com/) to learn about the project in detail, and follow my blog check out further tuorials, I will keep on posting new content.\n",
        "\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- Jupyter IPython Installed\n",
        "- Download and Install Version 0.4 of Pytorch\n",
        "- Training Data txt file included along with IPython Notebook\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e-ahGouiq8Y",
        "colab_type": "text"
      },
      "source": [
        "### using bcolz to store np.array in memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb4jgpUJ0xdF",
        "colab_type": "code",
        "outputId": "e66094c8-9509-417f-96c3-caeb74596090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "#bolz not available, install it \n",
        "!pip install bcolz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from bcolz) (1.16.5)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp36-cp36m-linux_x86_64.whl size=2661535 sha256=97544fc6a27eb217b4915dc9370a2d80c54a2fa4a06dfa4cd7f4ffdaf311d946\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n",
            "Installing collected packages: bcolz\n",
            "Successfully installed bcolz-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1yqLcoF0fto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import bcolz  # to process the data from Glove File \n",
        "import pickle # to dump and load pretrained glove vectors \n",
        "import copy   # to make deepcopy of python lists and dictionaries\n",
        "import operator\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcATHCssjSpm",
        "colab_type": "text"
      },
      "source": [
        "## Running the Ipython Notebook on Google Colaboratory\n",
        "\n",
        "### \" I have kept the dataset and Glove6B file in Gdrive and will access those files from Gdrive only\"\n",
        "\n",
        "### Mounting Google Drive Now !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO2GbIj21c84",
        "colab_type": "code",
        "outputId": "8d7d319a-3f64-4659-bd28-68e9890a157f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OwDNNus0ftq",
        "colab_type": "text"
      },
      "source": [
        "# Loading Glove Word Vectors\n",
        "\n",
        "Initially using 50 dimensional vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbLtCnl60ftr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = bcolz.carray(np.zeros(1), rootdir=f'/gdrive/My Drive/glove.6B/6B.50.dat', mode='w')\n",
        "\n",
        "with open(f'/gdrive/My Drive/glove.6B/glove.6B.50d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors.append(vect)\n",
        "    \n",
        "vectors = bcolz.carray(vectors[1:].reshape((400000, 50)), rootdir=f'/gdrive/My Drive/glove.6B/6B.50.dat', mode='w')\n",
        "vectors.flush()\n",
        "pickle.dump(words, open(f'/gdrive/My Drive/glove.6B/6B.50_words.pkl', 'wb'))\n",
        "pickle.dump(word2idx, open(f'/gdrive/My Drive/glove.6B/6B.50_idx.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QniCyjbk0ftu",
        "colab_type": "text"
      },
      "source": [
        "Using those objects we can now create a dictionary that given a word returns its vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEdAaTh7Z9C4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = bcolz.open(f'/gdrive/My Drive/glove.6B/6B.50.dat')[:]\n",
        "words = pickle.load(open(f'/gdrive/My Drive/glove.6B/6B.50_words.pkl', 'rb'))\n",
        "word2idx = pickle.load(open(f'/gdrive/My Drive/glove.6B/6B.50_idx.pkl', 'rb'))\n",
        "\n",
        "glove = {w: vectors[word2idx[w]] for w in words}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGUa8cFve9Lh",
        "colab_type": "text"
      },
      "source": [
        "### for Later processing\n",
        "###     - 'sos' token to be at index = 0\n",
        "###     - 'eos' token to be at index = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tsTo5l7aqYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sos_index = word2idx['sos']\n",
        "eos_index = word2idx['eos']\n",
        "sos_swap_word = words[0]\n",
        "eos_swap_word = words[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra6eNdLQfnBC",
        "colab_type": "text"
      },
      "source": [
        "### Swapping 'sos' token index and 'eos' token index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI4ecn4wbbkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words[0], words[sos_index] = words[sos_index], words[0]\n",
        "words[1], words[eos_index] = words[eos_index], words[1]\n",
        "word2idx[sos_swap_word], word2idx['sos'] = word2idx['sos'], word2idx[sos_swap_word]\n",
        "word2idx[eos_swap_word], word2idx['eos'] = word2idx['eos'], word2idx[eos_swap_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieS5fgsh0ft4",
        "colab_type": "text"
      },
      "source": [
        "# Creating Sorted Instance of word2idx "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qfUpiMgue43K",
        "colab": {}
      },
      "source": [
        "word2idx = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUvr99x30ft6",
        "colab_type": "text"
      },
      "source": [
        "Loading data files\n",
        "==================\n",
        "\n",
        "The data for this project is a set of Three Hundred and Fifty Question and Answer Pairs \n",
        "The file is a tab separated list of Question Answer Pairs:\n",
        "\n",
        "::\n",
        "\n",
        "    Amazon EC2s simple web service interface allows you to obtain and configure capacity with minimal friction. \tWhat can I do with Amazon EC2\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM9qzgQc0ft7",
        "colab_type": "text"
      },
      "source": [
        "each word in training set is represented as a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDXnZwpS0ft8",
        "colab_type": "text"
      },
      "source": [
        "We'll need a unique index per word to use as the inputs and targets of\n",
        "the networks later. To keep track of all this we will use a helper class\n",
        "called ``Lang`` which has word → index (``word2index``) and index → word\n",
        "(``index2word``) dictionaries, as well as a count of each word\n",
        "``word2count`` to use to later replace rare words.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu_QulvU0ft9",
        "colab_type": "text"
      },
      "source": [
        "# Creating 2 Seperate Classes for Input Lang and Ouput Lang\n",
        "\n",
        "Class 1 - > InputLang - built from Glove \n",
        "\n",
        "Class 2 - > OutputLang - built from Target Dictionary \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShnUThvq0ft-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputLang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}\n",
        "        self.word2count = { word : 1 for word in words }\n",
        "        self.index2word = { i : word for word, i in word2idx.items() }\n",
        "        self.n_words = 400001\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMSLSrFo0fuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class OutputLang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v29S8Qj0fuD",
        "colab_type": "text"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDc9n2350fuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAejAP1e0fuG",
        "colab_type": "text"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all Answer → Question, so if we\n",
        "want to translate from Question → Answer ``reverse``\n",
        "flag is there to reverse the pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPeu55EU0fuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangs():\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('/gdrive/My Drive/data.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "   \n",
        "    pairs = [list((p)) for p in pairs]\n",
        "\n",
        "    input_lang = InputLang('que')\n",
        "    output_lang = OutputLang('ans')\n",
        "    \n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6MBtQHU0fuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH \n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVALmAT80fuM",
        "colab_type": "text"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77l2ynUU0fuN",
        "colab_type": "code",
        "outputId": "2a0d4312-a01b-462d-fc57-90cd409375b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "def prepareData():\n",
        "    input_lang, output_lang, pairs = readLangs()\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[1])\n",
        "        output_lang.addSentence(pair[0])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData()\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 13 sentence pairs\n",
            "Trimmed to 13 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "que 400002\n",
            "ans 187\n",
            "['amazon ec registration requires you to have a valid phone number and email address on file with aws in case we ever need to contact you .', 'why am i asked to verify my phone number when signing up for amazon ec ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd8UhKQsiHlC",
        "colab_type": "text"
      },
      "source": [
        "## Initializing weight Matrix "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIE286ul0fuR",
        "colab_type": "text"
      },
      "source": [
        "We must build a matrix of weights that will be loaded into the PyTorch embedding layer. Its shape will be equal to:\n",
        "# (dataset’s vocabulary length, word vectors dimension)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0vq8Bnf0fuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix_len = input_lang.n_words\n",
        "\n",
        "weights_matrix = np.zeros((matrix_len, 50))\n",
        "words_found = 0\n",
        "\n",
        "for i, word in enumerate(input_lang.word2index):\n",
        "    try: \n",
        "        weights_matrix[i] = glove[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(50, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB67Uqvk0fuY",
        "colab_type": "text"
      },
      "source": [
        "The Seq2Seq Model\n",
        "=================\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
        "seq2seq network, or `Encoder Decoder` is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for NLP tasks consisting long Sentences.\n",
        "\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdLcGzj90fuZ",
        "colab_type": "text"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnFBJUFO0fuV",
        "colab_type": "text"
      },
      "source": [
        "### Encoder with Pre Trained Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0FNE4vw0fuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = embedding_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXVW27j-0fug",
        "colab_type": "text"
      },
      "source": [
        "The Decoder\n",
        "-----------\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the Answer .\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Od67fd0fui",
        "colab_type": "text"
      },
      "source": [
        "Simple Decoder\n",
        "\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QOEffPx0ful",
        "colab_type": "text"
      },
      "source": [
        "We have not used the Decoder Model here, and straight away used Attension Decoder Model explained below\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMiyDcuh0ful",
        "colab_type": "text"
      },
      "source": [
        "Attention Decoder\n",
        "******************\n",
        "\n",
        "If only the context vector is passed betweeen the encoder and decoder,\n",
        "that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to \"focus\" on a different part of\n",
        "the encoder's outputs for every step of the decoder's own outputs. First\n",
        "we calculate a set of *attention weights*. These will be multiplied by\n",
        "the encoder output vectors to create a weighted combination. The result\n",
        "(called ``attn_applied`` in the code) should contain information about\n",
        "that specific part of the input sequence, and thus help the decoder\n",
        "choose the right output words.\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward\n",
        "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
        "Because there are sentences of all sizes in the training data, to\n",
        "actually create and train this layer we have to choose a maximum\n",
        "sentence length (input length, for encoder outputs) that it can apply\n",
        "to. Sentences of the maximum length will use all the attention weights,\n",
        "while shorter sentences will only use the first few.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxr_ggk50fum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESG_o1K30fuo",
        "colab_type": "text"
      },
      "source": [
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-6pU9RY0fup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[1])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[0])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdeXIB-f0fur",
        "colab_type": "text"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but `when the trained\n",
        "network is exploited, it may exhibit instability `\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KbIxZ3i0fus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgVLIf_j0fuv",
        "colab_type": "text"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfOYmvS60fuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGd2uPI-0fuy",
        "colab_type": "text"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgXFYrmX0fuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    \n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        \n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTATpy-r0fu5",
        "colab_type": "text"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG6_DH7d0fu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A36wNnJ90fvB",
        "colab_type": "text"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4YSXB9_0fvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[1])\n",
        "        print('=', pair[0])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[1])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElRH5aM_0fvF",
        "colab_type": "text"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n",
        "With all these helper functions in place we can actually\n",
        "initialize a network and start training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffa5G0Yl0fvG",
        "colab_type": "code",
        "outputId": "a1637dc7-9936-44ae-ae91-b150a3cbafdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "hidden_size = 50\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder, attn_decoder, 30000, print_every=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 40s (- 48m 29s) (1000 3%) 3.0725\n",
            "3m 26s (- 48m 8s) (2000 6%) 1.9755\n",
            "5m 16s (- 47m 30s) (3000 10%) 0.7295\n",
            "7m 8s (- 46m 25s) (4000 13%) 0.1625\n",
            "9m 1s (- 45m 5s) (5000 16%) 0.0343\n",
            "10m 55s (- 43m 42s) (6000 20%) 0.0174\n",
            "12m 49s (- 42m 8s) (7000 23%) 0.0126\n",
            "14m 43s (- 40m 28s) (8000 26%) 0.0098\n",
            "16m 38s (- 38m 50s) (9000 30%) 0.0081\n",
            "18m 30s (- 37m 1s) (10000 33%) 0.0069\n",
            "20m 22s (- 35m 11s) (11000 36%) 0.0060\n",
            "22m 14s (- 33m 22s) (12000 40%) 0.0052\n",
            "24m 8s (- 31m 34s) (13000 43%) 0.0049\n",
            "26m 1s (- 29m 44s) (14000 46%) 0.0043\n",
            "27m 54s (- 27m 54s) (15000 50%) 0.0039\n",
            "29m 46s (- 26m 2s) (16000 53%) 0.0035\n",
            "31m 39s (- 24m 12s) (17000 56%) 0.0034\n",
            "33m 30s (- 22m 20s) (18000 60%) 0.0036\n",
            "35m 21s (- 20m 28s) (19000 63%) 0.0029\n",
            "37m 16s (- 18m 38s) (20000 66%) 0.0027\n",
            "39m 8s (- 16m 46s) (21000 70%) 0.0026\n",
            "41m 0s (- 14m 54s) (22000 73%) 0.0024\n",
            "42m 52s (- 13m 2s) (23000 76%) 0.0023\n",
            "44m 44s (- 11m 11s) (24000 80%) 0.0022\n",
            "46m 36s (- 9m 19s) (25000 83%) 0.0021\n",
            "48m 28s (- 7m 27s) (26000 86%) 0.0020\n",
            "50m 21s (- 5m 35s) (27000 90%) 0.0019\n",
            "52m 15s (- 3m 43s) (28000 93%) 0.0018\n",
            "54m 5s (- 1m 51s) (29000 96%) 0.0017\n",
            "55m 59s (- 0m 0s) (30000 100%) 0.0017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9RXCXB00fvI",
        "colab_type": "code",
        "outputId": "f1100ca6-1da3-439b-ac08-574be41ff64a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "evaluateRandomly(encoder, attn_decoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> is amazon ec used in conjunction with amazon s \n",
            "= yes amazon ec is used jointly with amazon s for instances with root devices backed by local instance storage .\n",
            "< yes amazon ec is used jointly with amazon s for instances with root devices backed by local instance storage . <EOS>\n",
            "\n",
            "> what can i do with amazon ec \n",
            "= amazon ec s simple web service interface allows you to obtain and configure capacity with minimal friction .\n",
            "< amazon ec s simple web service interface allows you to obtain and configure capacity with minimal friction . <EOS>\n",
            "\n",
            "> what is amazon elastic compute cloud amazon ec \n",
            "= amazon elastic compute cloud amazon ec is a web service that provides resizable compute capacity in the cloud . it is designed to make web scale computing easier for developers .\n",
            "< amazon elastic compute cloud amazon ec is a web service that provides resizable compute capacity in the cloud . it is designed to make web scale computing easier for developers . <EOS>\n",
            "\n",
            "> how do i load and store my systems with amazon ec \n",
            "= amazon ec allows you to set up and configure everything about your instances from your operating system up to your applications . an amazon machine image ami is simply a packaged up environment that includes all the necessary bits to set up and boot your instance .\n",
            "< amazon ec allows you to set up and configure everything about your instances from your operating system up to your applications . an amazon machine image ami is simply a packaged up environment that includes all the necessary bits to set up and boot your instance . <EOS>\n",
            "\n",
            "> how do i run systems in the amazon ec environment\n",
            "= once you have set up your account and select or create your amis you are ready to boot your instance . you can start your ami on any number of on demand instances by using the runinstances api call .\n",
            "< once you have set up your account and select or create your amis you are ready to boot your instance . you can start your ami on any number of on demand instances by using the runinstances api call . <EOS>\n",
            "\n",
            "> what is the difference between using the local instance store and amazon elastic block store amazon ebs for the root device\n",
            "= when you launch your amazon ec instances you have the ability to store your root device data on amazon ebs or the local instance store . by using amazon ebs data on the root device will persist independently from the lifetime of the instance .\n",
            "< when you launch your amazon ec instances you have the ability to store your root device data on amazon ebs or the local instance store . by using amazon ebs data on the root device will persist independently from the lifetime of the instance . <EOS>\n",
            "\n",
            "> are there any limitations in sending email from amazon ec instances\n",
            "= yes in order to maintain the quality of amazon ec addresses for sending email we enforce default limits on the amount of email that can be sent from ec accounts .\n",
            "< yes in order to maintain the quality of amazon ec addresses for sending email we enforce default limits on the amount of email that can be sent from ec accounts . <EOS>\n",
            "\n",
            "> why am i asked to verify my phone number when signing up for amazon ec \n",
            "= amazon ec registration requires you to have a valid phone number and email address on file with aws in case we ever need to contact you .\n",
            "< amazon ec registration requires you to have a valid phone number and email address on file with aws in case we ever need to contact you . <EOS>\n",
            "\n",
            "> how many instances can i run in amazon ec \n",
            "= you are limited to running up to a total of on demand instances across the instance family purchasing reserved instances and requesting spot instances per your dynamic spot limit per region .\n",
            "< you are limited to running up to a total of on demand instances across the instance family purchasing reserved instances and requesting spot instances per your dynamic spot limit per region . <EOS>\n",
            "\n",
            "> is amazon ec used in conjunction with amazon s \n",
            "= yes amazon ec is used jointly with amazon s for instances with root devices backed by local instance storage .\n",
            "< yes amazon ec is used jointly with amazon s for instances with root devices backed by local instance storage . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRKrrfnLhc12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_input(encoder, decoder, input_sentence):\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_sentence)\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    print('>', output_sentence)\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJL_i2FsiCjy",
        "colab_type": "code",
        "outputId": "c47fef4d-b8d3-4f7f-ffe2-bc284ccb44ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "evaluate_input(encoder, attn_decoder,' how quickly will systems be running')\n",
        "evaluate_input(encoder, attn_decoder,' how fast will systems be running')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> it typically takes less than minutes from the issue of the runinstances call to the point where all requested instances begin their boot sequences . <EOS>\n",
            "\n",
            "> it typically takes less than minutes from the issue of the runinstances call to the point where all requested instances begin their boot sequences . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd5HEUjxCEx5",
        "colab_type": "code",
        "outputId": "ff1f50da-bf44-414c-835d-ddecfa0ba4c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "evaluate_input(encoder, attn_decoder,'how do i run systems in the amazon ec environment')\n",
        "evaluate_input(encoder, attn_decoder,'how do i launch systems in the amazon ec environment')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> once you have set up your account and select or create your amis you are ready to boot your instance . you can start your ami on any number of on demand instances by using the runinstances api call . <EOS>\n",
            "\n",
            "> once you have set up your account and select or create your amis you are ready to boot your instance . you can start your ami on any number of on demand instances by using the runinstances api call . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jEJKmpGJish",
        "colab_type": "code",
        "outputId": "aa8e8870-7f93-43f2-e296-e80a2bcd9170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "evaluate_input(encoder, attn_decoder,'is amazon ec used in conjunction with amazon s ')\n",
        "evaluate_input(encoder, attn_decoder,'is amazon ec used along with amazon s ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> yes amazon ec is used jointly with amazon s for instances with root devices backed by local instance storage . <EOS>\n",
            "\n",
            "> yes amazon ec is used jointly with amazon s for instances with root devices backed by local instance storage . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzDpzN_RM3qg",
        "colab_type": "code",
        "outputId": "4e809c62-8c7a-4941-b119-d3f473e05072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "evaluate_input(encoder, attn_decoder,'are there any limitations in sending email from amazon ec instances')\n",
        "evaluate_input(encoder, attn_decoder,'what are the disadvantages in sending email from amazon ec instances')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> yes in order to maintain the quality of amazon ec addresses for sending email we enforce default limits on the amount of email that can be sent from ec accounts . <EOS>\n",
            "\n",
            "> yes in order to maintain the quality of amazon ec addresses for sending email we enforce default limits on the amount of email that can be sent from ec accounts . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "482y8AJqNbVD",
        "colab_type": "code",
        "outputId": "b51f3107-d1e1-45e7-fafc-288ce69795f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "evaluate_input(encoder, attn_decoder,'why am i asked to verify my phone number when signing up for amazon ec ')\n",
        "evaluate_input(encoder, attn_decoder,'why am i asked to confirm my phone number when signing up for amazon ec ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> amazon ec registration requires you to have a valid phone number and email address on file with aws in case we ever need to contact you . <EOS>\n",
            "\n",
            "> amazon ec registration requires you to have a valid phone number and email address on file with aws in case we ever need to contact you . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}